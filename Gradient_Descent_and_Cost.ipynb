{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFIzaPFXuUjP+3zyHkj0mh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahi0097/ML_Learning/blob/main/Gradient_Descent_and_Cost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UNmlDykT9fB",
        "outputId": "712ece23-56ad-40ba-e975-7677a8480535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: cost_fun = 89.0000, md_fun = -62.0000, bd_fun = -18.0000, m_curr = 0.6200, b_curr = 0.1800\n",
            "Iteration 2: cost_fun = 52.2504, md_fun = -47.2800, bd_fun = -13.9200, m_curr = 1.0928, b_curr = 0.3192\n",
            "Iteration 3: cost_fun = 30.8319, md_fun = -36.0432, bd_fun = -10.8048, m_curr = 1.4532, b_curr = 0.4272\n",
            "Iteration 4: cost_fun = 18.3478, md_fun = -27.4654, bd_fun = -8.4261, m_curr = 1.7279, b_curr = 0.5115\n",
            "Iteration 5: cost_fun = 11.0700, md_fun = -20.9175, bd_fun = -6.6097, m_curr = 1.9371, b_curr = 0.5776\n",
            "Iteration 6: cost_fun = 6.8264, md_fun = -15.9190, bd_fun = -5.2224, m_curr = 2.0963, b_curr = 0.6298\n",
            "Iteration 7: cost_fun = 4.3508, md_fun = -12.1035, bd_fun = -4.1628, m_curr = 2.2173, b_curr = 0.6715\n",
            "Iteration 8: cost_fun = 2.9057, md_fun = -9.1910, bd_fun = -3.3534, m_curr = 2.3092, b_curr = 0.7050\n",
            "Iteration 9: cost_fun = 2.0610, md_fun = -6.9677, bd_fun = -2.7348, m_curr = 2.3789, b_curr = 0.7323\n",
            "Iteration 10: cost_fun = 1.5663, md_fun = -5.2708, bd_fun = -2.2621, m_curr = 2.4316, b_curr = 0.7550\n",
            "(np.float64(2.431580493177024), np.float64(0.7549612843324961))\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "def Gradient_Desent(x,y):\n",
        "  n = len(x)\n",
        "  m_curr= b_curr = 0\n",
        "  interation = 10\n",
        "  learning_rate = 0.01\n",
        "\n",
        "\n",
        "  for i in  range(interation):\n",
        "     #finding predict value\n",
        "     yp = m_curr * x + b_curr\n",
        "     #finding cost function\n",
        "     cost_fun = (1/n) * sum((y - yp) ** 2)\n",
        "     #Slope gradient\n",
        "     md_fun = -(2/n) *sum(x*(y-yp))\n",
        "     #Intercept gradient\n",
        "     bd_fun = -(2/n) *sum((y-yp))\n",
        "     # current guess of the slope (how much y increases with x)\n",
        "     m_curr = m_curr -  learning_rate * md_fun\n",
        "     #current guess of the intercept (starting point when x = 0)\n",
        "     b_curr = b_curr -  learning_rate * bd_fun\n",
        "     print(\"Iteration {}: cost_fun = {:.4f}, md_fun = {:.4f}, bd_fun = {:.4f}, m_curr = {:.4f}, b_curr = {:.4f}\".format(\n",
        "            i+1, cost_fun, md_fun, bd_fun, m_curr, b_curr))\n",
        "\n",
        "  return m_curr, b_curr\n",
        "\n",
        "x = np.array([1,2,3,4,5])\n",
        "y = np.array([5,7,9,11,13])\n",
        "print(Gradient_Desent(x,y))"
      ]
    }
  ]
}